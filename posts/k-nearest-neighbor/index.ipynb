{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # k-Nearest Neighbor\n",
    "\n",
    " k Nearest Neighbour (kNN) is a non-parametric algorithm for classification or regression algorithm. This is usually the  first choices for a classification study when there is little or no prior knowledge about the distribution of the data. In kNN, the analysis is performed where parameteric estimates of probablity density are unknown or difficult to determine.\n",
    "\n",
    " ![kNN visualization](https://upload.wikimedia.org/wikipedia/commons/e/e7/KnnClassification.svg \"kNN visualization\")\n",
    "\n",
    " The kNN algorithm is fed some training data. This traning data has *h* *n* dimentional data points with the associated lables. When a new *n* dimentional data point i.e. *test* data point is provided, the distance between the test point and each of the *h* training points is calculated. This distance is usually Euclidean distance. Then the distances are sorted from the smallest to the largest. Then the majority of the *k* smallest point's lable is assigned to the test point.\n",
    "\n",
    " This can be represented as:\n",
    "\n",
    " Classification typically involves partitioning samples into training and testing categories. Let $$x_i$$ be a training sample and x be a test sample, and let $$\\omega$$ be the true class of a training sample and $$ \\hat{\\omega}$$ be the predicted class for a test sample $$(\\omega, \\hat{\\omega}=1,2,â€¦,\\Omega)$$ . Here, $$\\Omega$$ is the total number of classes.\n",
    "\n",
    " During the training process, we use only the true class $$\\Omega$$ of each training sample to train the classifier, while during testing we predict the class $$ \\hat{\\omega}$$ of each test sample. It warrants noting that kNN is a \"supervised\" classification method in that it uses the class labels of the training data.\n",
    "\n",
    " With 1-nearest neighbor rule, the predicted class of test sample x is set equal to the true class $$\\omega$$ of its nearest neighbor, where $$m_i$$ is a nearest neighbor to x if the distance\n",
    "\n",
    " $$d(m_i,x)=min_j{d(m_j,x)}$$.\n",
    "\n",
    " For k-nearest neighbors, the predicted class of test sample x is set equal to the most frequent true class among k nearest training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class kNN:\n",
    "    \"\"\"k-Nearest Neighbor implements the bruteforce kNN classification algorithm.\n",
    "    \n",
    "    Attributes:\n",
    "        x_train (:obj:ndarray numpy array): The training feature array.\n",
    "        y_train (:obj:ndarray numpy array): The training lable array.\n",
    "        x_test (:obj:ndarray numpy array): The test feature array.\n",
    "        y_test (:obj:ndarray numpy array): The test lable array.\n",
    "        k (int): The k nearest neighbors that need to be looked at.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, k):\n",
    "        if not isinstance(k, int):\n",
    "            raise ValueError(\"The type of k should be an 'int' but found {}\".format(type(k)))\n",
    "        self.x_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.x_test = np.array([])\n",
    "        self.y_test = []\n",
    "        self._test_feature_length = 0\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        \n",
    "        if not isinstance(x_train, np.ndarray):\n",
    "            raise ValueError(\"The type of x_train should be an 'numpy.ndarray' but found {}\".format(type(x_train)))\n",
    "        if not isinstance(y_train, np.ndarray):\n",
    "            raise ValueError(\"The type of y_train should be an 'numpy.ndarray' but found {}\".format(type(y_train)))\n",
    "        \n",
    "        #Check if all the x_train points have the same features.\n",
    "        if len(x_train.shape) == 1:\n",
    "            raise ValueError(\"The traning points are not of the same size. The number of feature are different.\")\n",
    "        \n",
    "        if not x_train.shape[0] == y_train.size:\n",
    "            raise ValueError(\"The number of training examples and the lables are not of the name size.\")\n",
    "        \n",
    "        self._test_feature_length = x_train.shape[1]\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        if not isinstance(x_test, np.ndarray):\n",
    "            raise ValueError(\"The type of x_test should be an 'numpy.ndarray' but found {}\".format(type(x_test)))\n",
    "        \n",
    "        if len(x_test.shape) == 1:\n",
    "            raise ValueError(\"The testing points are not of the same size. The number of feature are different.\")\n",
    "        \n",
    "        if not x_test.shape[1] == self._test_feature_length:\n",
    "            raise ValueError(\"The test and training feature sets are not the same.  Training feature length {}, Test feature length {}\".format(_test_feature_length, x_test.shape[1]))\n",
    "        \n",
    "        y_pred = []\n",
    "        \n",
    "        for x_ in  x_test:\n",
    "            _distLst = []\n",
    "            for i, x in enumerate(self.x_train):\n",
    "                _distLst.append({\"index\": i ,\"dist\" : distance.euclidean(x, x_)})\n",
    "            _distLst = sorted(_distLst, key = lambda x:x['dist'])\n",
    "            _distLst = _distLst[:self.k] #Select only first k lables with the shortest distance\n",
    "            \n",
    "            kNN = []\n",
    "            for ele in _distLst:\n",
    "                kNN.append(self.y_train[ele[\"index\"]])\n",
    "            \n",
    "            \n",
    "        \n",
    "        return Counter(kNN).most_common(1)[0][0]\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0], [1], [2], [3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "k = kNN(3)\n",
    "\n",
    "k.fit(X,y)\n",
    "k.predict(np.array([[1.1]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "nikola": {
   "category": "",
   "date": "2019-05-19 19:57:26 UTC+05:30",
   "description": "",
   "link": "",
   "slug": "k-nearest-neighbor",
   "tags": "",
   "title": "k-Nearest Neighbor",
   "type": "text"
  },
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
