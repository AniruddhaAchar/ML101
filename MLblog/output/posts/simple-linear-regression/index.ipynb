{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Simple Linear regression\n",
    " This is amongst the simplest and popular regression algorithm. This like almost all machine learning algorithm has a strong start in statics.\n",
    " This algorithm, is used to map the relationship between two variables namely **X** and **Y**.\n",
    " Given known values of X and Y; assuming that the relation between the variables is linear in nature, can we fit a line that predicts values of Y based on given values of X?\n",
    " Well... this is what is achieved by linear regression. With this definition, we know the limitation. This can only fit observations with linear relationships.\n",
    "\n",
    "Talking about lines, one of the most familiar equations in math springs to mind.\n",
    "$$ y = mx +c$$\n",
    "\n",
    "The above equation represents shows that we can predict the value of the unknown/dependent values of **Y** if we know the right combination of __m__ and __c__.\n",
    "Here __m__ is called the scale factor or bias and __c__ is called the bias coefficient.\n",
    "So how do we compute the right values of __m__ and __c__? There are two approaches to get these values.\n",
    "+ Gradient decent\n",
    "+ Least Mean Squared Method.\n",
    "\n",
    "For this post, we will be using the Least Mean Squared Method. Our aim here is to reduce the difference between the actual value of *y* and the predicted value. Lets called the predicted value *h(x)*\n",
    "For an instance of *i* of *y*\n",
    "$$y_i = mx_i + c + \\epsilon_i$$\n",
    "Here $$\\epsilon_i$$ is the error in computation of **y_i**. Our learning algorithm's main task is to learn the values of *m* and *c* so that $\\epsilon$ is minimum. This minimization is inferred using the cost function which is given by\n",
    "$$J(m, c) = \\frac{1}{2n}\\sum_{i=1}^{n}\\epsilon_i^2$$\n",
    "Our task here is to minimize the cost function defined above. We can do that through gradient decent as mentioned above, we have can also use a less computational method (is not that accurate) which is the least mean squared method.\n",
    "Not going into a lot of math, we derive the values of *m* and *c* as\n",
    "$$c = \\frac{SS_{xy}}{SS_{xx}}$$\n",
    "$$m = \\overline{y} - c\\overline{x}$$\n",
    "Here $$ \\overline{y} and \\overline{x} $$ are mean/arithemetic averages of *y* and *x* respectively.\n",
    "$${SS_{xy}} = \\sum_{i=1}^{n}(x_i - \\overline{x})(y_i - \\overline{y}) = \\sum_{i=1}^{n}x_iy_i - n\\overline{x}\\overline{y}$$\n",
    "$${SS_{xx}} = \\sum_{i=1}^{n}(x_i - \\overline{x})^2 = \\sum_{i=1}^{n}x_i^2 - n(\\overline{x})^2$$\n",
    "\n",
    "Done with all the math talk. Lets implement this in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class linearRegression():\n",
    "    \"\"\"Linear Regression computes linear regression line using Least Squared Method. \n",
    "        This implementation is for a univalue training and test data.\n",
    "    \n",
    "    Attributes:\n",
    "        x_train (:obj:1darray numpy array): The training feature array.\n",
    "        y_train (:obj:1darray numpy array): The training label array.\n",
    "        x_test (:obj:1darray numpy array): The test feature array.\n",
    "        y_test (:obj:1darray numpy array): The test label array.\n",
    "        m (float): Initial value of first coefficient. Optional.\n",
    "        c (float): Initial value of first coefficient. Optional.\n",
    "    \"\"\"\n",
    "    def __init__(self, m = 0.0, c = 0.0):\n",
    "        if not isinstance(m, float):\n",
    "            raise ValueError(\"The type of m should be an 'float' but found {}\".format(type(m)))\n",
    "        if not isinstance(c, float):\n",
    "            raise ValueError(\"The type of c should be an 'float' but found {}\".format(type(c)))\n",
    "        self.x_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.x_test = np.array([])\n",
    "        self.y_test = []\n",
    "        self.m = m\n",
    "        self.c = c\n",
    "    def fit(self, x_train, y_train):\n",
    "\n",
    "        \"\"\"Fits the simple linear regression model. Does some validation on the data passed to it.\n",
    "        Args:\n",
    "        x_train (numpy ndarray): The independent variables.\n",
    "        y_train (numpy ndarray): The dependent variable.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If x_train is not of type numpy.\n",
    "            ValueError: If y_train is not of type numpy.\n",
    "            ValueError: If x_train is not a one dimensional array.\n",
    "            ValueError: If y_train is not a one dimensional array.\n",
    "            ValueError: If the length of x_train and y_train are not same.\n",
    "        \"\"\"\n",
    "            \n",
    "        if not isinstance(x_train, np.ndarray):\n",
    "            raise ValueError(\"The type of x_train should be an 'numpy.ndarray' but found {}\".format(type(x_train)))\n",
    "        if not isinstance(y_train, np.ndarray):\n",
    "            raise ValueError(\"The type of y_train should be an 'numpy.ndarray' but found {}\".format(type(y_train)))\n",
    "        \n",
    "        if(x_train.ndim > 1):\n",
    "            raise ValueError(\"This implementation only calculates univalue linear regression line. We found dimension as {}\".format(x_train.ndim))\n",
    "        \n",
    "        if(y_train.ndim > 1):\n",
    "            raise ValueError(\"The dependent/target training value must be a one dimensional array. We found dimension as {}\".format(y_train.ndim))\n",
    "        \n",
    "        if not x_train.shape[0] == y_train.size:\n",
    "            raise ValueError(\"The number of training examples and the lables are not of the name size.\")\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.m, self.c = self.compute_coef(self.x_train, self.y_train)\n",
    "\n",
    "    def compute_coef(self, x, y):\n",
    "\n",
    "        \"\"\"Computes the regression coefficients for the given values of x and y.\n",
    "\n",
    "        Args:\n",
    "            x (numpy ndarray): The independent variables.\n",
    "            y (numpy ndarray): The dependent variable.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            (m, c) (:tuple:float): The computed regression coefficients.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of observations/points \n",
    "        n = np.size(x) \n",
    "\n",
    "        # mean of x and y\n",
    "        m_x, m_y = np.mean(x), np.mean(y)\n",
    "\n",
    "        #compute the cross-deviation and deviation of x\n",
    "        SS_xy = np.sum(y*x) - n*m_y*m_x \n",
    "        SS_xx = np.sum(x*x) - n*m_x*m_x \n",
    "\n",
    "        #Compute the regression coefficients\n",
    "        c = SS_xy / SS_xx \n",
    "        m = m_y - c*m_x \n",
    "        \n",
    "        return (m, c)\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        \"\"\"Predicts the y_test values using the calculated coefficients.\n",
    "        \n",
    "        Arguments:\n",
    "            x_test {numpy ndarray} -- The x values on which the predictions needs to be made.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If x_train is not of type numpy ndarray\n",
    "        \n",
    "        Return:\n",
    "            y_pred {list: float} -- The predicted values.\n",
    "        \"\"\"\n",
    "        if not isinstance(x_test, np.ndarray):\n",
    "            raise ValueError(\"The type of x_test should be an 'numpy.ndarray' but found {}\".format(type(x_test)))\n",
    "        \n",
    "        \n",
    "        y_pred = []\n",
    "        \n",
    "        for _x in x_test:\n",
    "            y_pred.append(self.m*_x + self.c)\n",
    "        \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1696969696969697, 2.4060606060606062, 3.6424242424242426, 4.878787878787879, 6.115151515151515, 7.351515151515152, 8.587878787878788, 9.824242424242424, 11.06060606060606, 12.296969696969697]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12]) \n",
    "model = linearRegression()\n",
    "model.fit(x,y)\n",
    "print(model.predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "nikola": {
   "category": "",
   "date": "2019-05-19 19:51:26 UTC+05:30",
   "description": "",
   "link": "",
   "slug": "simple-linear-regression",
   "tags": "",
   "title": "Simple Linear regression",
   "type": "text"
  },
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
