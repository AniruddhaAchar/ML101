{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# True Positive, True Negative, False Negative and False Positive\n\nIn classification, the prediction of a model can be generally catogarized into True Positive, True Negative, False Negative and False Positive where\n\n1. True Positive is an outcome where the model correctly predicts the positive class.\n2. True Negative is an outcome where the model correctly predicts the negative class.\n3. Flase Positive is an outcome where the model incorrectly predicts the positive class.\n4. Flase Negative is an outcome where the model incorrectly predicts the negative class.\n\nThis can be represented in a matrix as:\n\n|    |    |\n|----|----|\n| TP | FP |\n| FN | TN |"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ndef get_positives_and_negatives(yTrue, yPredicted):\n    \"\"\"Calculates the True Positive, True Negative, False Negative and False Positive of the given true and predicted numpy arrays.\n        Here the arrays are assumed to have only 2 classes.\n    \n    Args:\n        yTrue (numpy ndarray): The true lables. Ground truth.\n        yPredict (numpy ndarray): The predicted lables. Predictions.\n    \n    Raises:\n        Assertion Exception: If yTrue or yPredict is not a numpy array\n    \n    Returns (:obj: numpy ndarray): A numpy array containing True Positive, True Negative, False Negative and False Positive\n    \n    Example:\n        get_positives_and_negatives(np.asarray([0,0,1,0,0,1,0]), np.asarray([0,1,1,0,1,0,0]))\n    \n    \"\"\"\n    \n    if not isinstance(yTrue, np.ndarray):\n        raise AssertionError(\"{} must be of type numpy.ndarray\".format(yTrue))\n    if not isinstance(yPredicted, np.ndarray):\n        raise AssertionError(\"{} must be of type numpy.ndarray\".format(yPredicted))\n    \n    TP = np.sum(np.logical_and(yPredicted == 1, yTrue == 1))\n    \n    TN = np.sum(np.logical_and(yPredicted == 0, yTrue == 0))\n    \n    FP = np.sum(np.logical_and(yPredicted == 1, yTrue == 0))\n    \n    FN = np.sum(np.logical_and(yPredicted == 0, yTrue == 1))\n    \n    return np.array([TP, TN, FP, FN])",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "get_positives_and_negatives(np.asarray([0,0,1,0,0,1,0]), np.asarray([0,1,1,0,1,0,0]))\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "array([1, 3, 2, 1])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Confusion matrix\n\nConfusion matrix is used to visualize the performance of a classification model. In this matrix, the rows are the predicted values and the columns are the true values. The name stems from the fact that it makes it easy to see if the model is confusing two classes.\n\n|        | Cat | Dog | Rabbit |\n|--------|-----|-----|--------|\n| Cat    | 5   | 2   | 0      |\n| Dog    | 3   | 3   | 0      |\n| Rabbit | 1   | 1   | 5      |"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom scipy.sparse import coo_matrix\n\ndef confusion_matrix(y_pred, y_true):\n    \"\"\"Builds a confusion matrix for the given predicted and trye numpy array.\n    \n    Args:\n        y_pred (numpy ndarray): The true lables. Ground truth.\n        y_true (numpy ndarray): The predicted lables. Predictions.\n    \n    Raises:\n        Assertion Exception: If yTrue or yPredict is not a numpy array\n    \n    Returns:\n        C (numpy ndarray): The confusion matrix that was calculated.\n    \n    Example:\n        y_true = [2, 0, 2, 2, 0, 1]\n        y_pred = [0, 0, 2, 2, 0, 2]\n        confusion_matrix(y_pred, y_True)\n    \n    \"\"\"\n    if not isinstance(y_true, np.ndarray):\n        raise AssertionError(\"{} must be of type numpy.ndarray\".format(y_true))\n    \n    if not isinstance(y_pred, np.ndarray):\n        raise AssertionError(\"{} must be of type numpy.ndarray\".format(y_pred))\n    \n    if not (y_pred.size == y_true.size):\n        raise AssertionError(\"The predicted array and the ground truth arrays must be of the same size\")\n    \n    _labelsT = np.unique(y_true) #Get the lables in the ground truth.\n    \n    labels = _labelsT #Create a array of lables.\n    \n    weights = np.ones(y_true.shape[0]) # Create an array of ones so that we can pass this as the basic building block to confusion matrix. \n    \n    num_labels = labels.size\n    \n    #Builds a dict that converts lables to indices. This will help us populate the confusion matrix.\n    \n    labels_to_index = dict((label, index) for index, label in enumerate(labels)) \n    \n    #Build y_pred_idx and y_true_idx numpy arrays that has the index equivalent of the lables\n    \n    y_pred_idx = np.array([labels_to_index.get(label, num_labels+1) for label in y_pred])\n    \n    y_true_idx = np.array([labels_to_index.get(label, num_labels+1) for label in y_true])\n    \n    ind = np.logical_and(y_pred_idx < num_labels, y_true_idx < num_labels)\n    \n    y_pred_idx = y_pred_idx[ind]\n    y_true_idx = y_true_idx[ind]\n    weights = weights[ind]\n    \n    CM = coo_matrix((weights, (y_true_idx, y_pred_idx)), shape = (num_labels, num_labels)).toarray()\n    return CM",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_true = np.array([2, 0, 2, 2, 0, 1])\ny_pred = np.array([0, 0, 2, 2, 0, 2])\nconfusion_matrix(y_pred, y_true)",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 49,
          "data": {
            "text/plain": "array([[2., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 2.]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "confusion_matrix(y_pred, y_true).ravel()",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "array([2, 0, 0, 0, 0, 1, 1, 0, 2], dtype=int8)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "confusion_matrix(np.array([1, 1, 1, 0]), np.array([0, 1, 0, 1]))",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "array([[0, 2],\n       [1, 1]], dtype=int8)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tn, fp, fn, tp = confusion_matrix(np.array([1, 1, 1, 0]), np.array([0, 1, 0, 1])).ravel()\n(tn, fp, fn, tp)",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "(0, 2, 1, 1)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Accuracy\n\nThis is the fraction of the predictioins that were predicted right. It is given by:\n\n$$\\frac{True Positive + True Negative}{True Positive + True Negative + False Negative + False positive}$$\n\nWhen working with class imbalence data i.e. where the number of examples of each class is different, accuracy doesn't always give the full story. Here the model has been trained on one class more than on another and the test data also has an imbalence thus resulting in higer accuracy. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Precision and Recall\n\nPrecision is the fraction of the predicted values that were true positive i.e. are actually correct.\n\nIt is defined as \n$$\\frac{True Positive}{True Positive + False Positive}$$\n\nRecall is the fraction of the predicteds that were identified correctly.\n\nIt is defined as:\n\n$$\\frac{True Positive}{True Positive + False Negative}$$\n\n**In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# F1 Score\n\nF1 score is a measure of a test's accuracy. It is the harmonic mean of Precision and Recall.\n\nIt is defined as:\n\n$$2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_precision_recall_f1score(y_pred, y_true):\n    \"\"\"Calculates precision, recall and f1score the given predicted and trye numpy array.\n    \n    Args:\n         yTrue (numpy ndarray): The true lables. Ground truth.\n        yPredict (numpy ndarray): The predicted lables. Predictions.\n    \n    Raises:\n        Assertion Exception: If yTrue or yPredict is not a numpy array\n    \n    Returns:\n        cr (dictionary of precision, recall and f1score for each of the label): The precision, recall and f1score for each of the label.\n    \n    Example:\n        y_true = [0, 1, 2, 2, 2]\n        y_pred = [[0, 0, 2, 2, 1]\n        get_precision_recall_f1score(y_pred, y_True)\n    \n    \"\"\"\n    CM = confusion_matrix(y_pred, y_true)\n    \n    labels = np.unique(y_true)\n    \n    cr = dict()\n    \n    for label in labels:\n        tp = CM[label, label]\n        fp = CM[:,label].sum() - tp\n        fn = CM[label,:].sum() -tp\n        prec = np.around(tp/(tp+fp), decimals = 2)\n        recall = np.around(tp/(tp+fn), decimals = 2)\n        if prec == 0 and recall == 0 :\n            f1 = 0.0\n        else:\n            f1 = np.around(2* (prec * recall)/(prec + recall), decimals = 2)\n        cr[label] = (prec, recall, f1)\n    \n    return cr\n    ",
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_true = np.array([0, 1, 2, 2, 2])\ny_pred = np.array([0, 0, 2, 2, 1])\nget_precision_recall_f1score(y_pred, y_true)",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 65,
          "data": {
            "text/plain": "{0: (0.5, 1.0, 0.67), 1: (0.0, 0.0, 0.0), 2: (1.0, 0.67, 0.8)}"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}